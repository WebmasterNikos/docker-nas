version: "3.8"

networks:
  home:
    driver: bridge

x-env: &env
  PUID: 1000
  PGID: 1000
  TZ: Europe/London

x-healthcheck: &healthcheck
  test: ["CMD-SHELL", "curl -f https://icanhazip.com/ || exit 1"]
  interval: 10s
  timeout: 3s
  retries: 5

x-common: &common
  environment:
    <<: *env
  restart: unless-stopped
  healthcheck:
    <<: *healthcheck
  networks:
    home:

services:
  traefik:
    image: traefik:latest
    container_name: traefik
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    ports:
      - 80:80
      - 443:443
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./config/traefik/traefik.yaml:/traefik.yaml:ro
      - ./config/traefik/acme.json:/acme.json
    environment:
      - CF_DNS_API_TOKEN=${CLOUDFLARE_TRAEFIK_TOKEN}
    networks:
      home:
    labels:
      - traefik.enable=true
      - traefik.http.middlewares.admin.basicauth.users=${TRAEFIK_BASIC_AUTH_USER}:${TRAEFIK_BASIC_AUTH_PASSWORD}
      - traefik.http.services.traefik-dashboard.loadbalancer.server.port=443
      - traefik.http.routers.traefik-dashboard.rule=Host(`lab.petalas.dev`)
      - traefik.http.routers.traefik-dashboard.entrypoints=websecure
      - traefik.http.routers.traefik-dashboard.service=api@internal
      - traefik.http.routers.traefik-dashboard.middlewares=admin
      - traefik.http.routers.traefik-dashboard.tls.certresolver=cloudflare
      - traefik.http.routers.traefik-dashboard.tls.domains[0].main=lab.petalas.dev
      - traefik.http.routers.traefik-dashboard.tls.domains[0].sans=*.lab.petalas.dev


  whoami:
    image: traefik/whoami
    container_name: whoami
    networks:
      home:
    labels:
      - traefik.enable=true
      - traefik.http.routers.whoami.service=whoami
      - traefik.http.routers.whoami.entrypoints=websecure
      - traefik.http.routers.whoami.rule=Host(`whoami.lab.petalas.dev`)
      - traefik.http.routers.whoami.tls=true
      - traefik.http.services.whoami.loadbalancer.server.port=80


  glances:
    image: nicolargo/glances:latest-full
    container_name: glances
    restart: unless-stopped
    pid: host
    network_mode: host
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./config/glances:/etc/glances
      - /mnt:/mnt:ro
    environment:
      TZ: Europe/London
      GLANCES_OPT: "-w"
    labels:
      - traefik.enable=false

  code-server:
    image: lscr.io/linuxserver/code-server:latest
    container_name: code-server
    <<: *common
    environment:
      <<: *env
      PROXY_DOMAIN: code-server.lab.petalas.dev
      DEFAULT_WORKSPACE: /git
      PASSWORD: ${CODE_SERVER_PASSWORD}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./config/code-server:/config
      - ${HOME}/git:/git
    ports:
      - 8443:8443
    healthcheck:
        <<: *healthcheck
        test: ["CMD-SHELL", "curl -f http://localhost:8443 || exit 1"]
    labels:
      - traefik.enable=true
      - traefik.http.routers.code-server.service=code-server
      - traefik.http.routers.code-server.entrypoints=websecure
      - traefik.http.routers.code-server.rule=Host(`code-server.lab.petalas.dev`)
      - traefik.http.routers.code-server.tls=true
      - traefik.http.services.code-server.loadbalancer.server.port=8443

  homepage:
    # image: ghcr.io/gethomepage/homepage:latest
    build:
      context: ./dockerfiles
      dockerfile: ./homepage
    container_name: homepage
    restart: unless-stopped
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "exit 0"] # TODO: figure out what kind of healthcheck we can do
    ports:
      - 3000:3000
    volumes:
      - ./config/homepage:/app/config
      - ./images:/app/public/images
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      home:
    labels:
      - traefik.enable=false


  plex:
    image: linuxserver/plex
    container_name: plex
    network_mode: host
    environment:
      <<: *env
      VERSION: docker
    restart: unless-stopped
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "curl -f https://plex.com/ || exit 1"]
    volumes:
      - ./config/plex:/config
      - ./config/plex/transcode:/transcode
      - /mnt/storage/tv:/tv
      - /mnt/storage/movies:/movies
      - /mnt/storage/music:/music
    labels:
      - traefik.enable=false
      - homepage.weight=1
      - homepage.group=calendar
      - homepage.name=plex
      - homepage.icon=plex.svg
      - homepage.href=http://192.168.1.102:32400
      - homepage.widget.type=plex
      - homepage.widget.url=http://192.168.1.102:32400
      - homepage.widget.key=${PLEX_API_KEY}

  sonarr:
    image: lscr.io/linuxserver/sonarr:develop
    container_name: sonarr
    <<: *common
    volumes:
      - ./config/sonarr:/config
      - ./scripts:/scripts:ro
      - /mnt/storage/:/data
    ports:
      - 32784:8989
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "curl -f http://localhost:8989/sonarr/ping || exit 1"]
    depends_on:
      qbittorrent:
        condition: service_healthy
      sabnzbd:
        condition: service_healthy
      prowlarr:
        condition: service_healthy
    labels:
      - traefik.enable=false
      - homepage.weight=3
      - homepage.group=media
      - homepage.name=sonarr
      - homepage.icon=sonarr.svg
      - homepage.href=http://192.168.1.102:32784
      - homepage.widget.type=sonarr
      - homepage.widget.url=http://192.168.1.102:32784
      - homepage.widget.key=${SONARR_API_KEY}
      - homepage.widget.enableQueue=true

  # nicotine:
  #   image: npetalas/nicotineplus:xpra
  #   container_name: nicotine
  #   restart: unless-stopped
  #   environment:
  #     <<: *env
  #     DARKMODE: true
  #     LOGIN: ${SOULSEEK_LOGIN}
  #     PASSW: ${SOULSEEK_PASSWORD}
  #   volumes:
  #     - ./config/nicotine:/root/.config/nicotine # (Optional) Save your config persistently
  #     - ./config/nicotine/data:/root/.local/share/nicotine #(Optional) Store your logs, database, and history
  #     - /mnt/storage/music:/music
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '0.5'
  #         memory: 2G
  #   network_mode: service:gluetun
  #   depends_on:
  #     gluetun:
  #       condition: service_healthy

  # gluetun:
  #   image: qmcgaw/gluetun
  #   container_name: gluetun
  #   cap_add:
  #     - NET_ADMIN
  #   devices:
  #     - /dev/net/tun:/dev/net/tun
  #   ports:
  #     - 8000:8000 # control server
  #     - 6565:6565 # nicotine+
  #     - ${VPN_FORWARDED_PORT}:${VPN_FORWARDED_PORT}
  #   volumes:
  #     - ./config/gluetun:/gluetun
  #   environment:
  #     <<: *env
  #     VPN_SERVICE_PROVIDER: private internet access
  #     OPENVPN_USER: ${VPN_USER}
  #     OPENVPN_PASSWORD: ${VPN_PASSWORD}
  #     SERVER_REGIONS: UK London
  #     VPN_PORT_FORWARDING: on
  #     PRIVATE_INTERNET_ACCESS_OPENVPN_ENCRYPTION_PRESET: 'normal' # strong|normal|none, default 'strong'
  #   networks:
  #    home:

  radarr:
    image: linuxserver/radarr
    container_name: radarr
    <<: *common
    volumes:
      - ./config/radarr:/config
      - ./scripts:/scripts:ro
      - /mnt/storage/:/data
    ports:
      - 32783:7878
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "curl -f http://localhost:7878/radarr/ping || exit 1"]
    depends_on:
      qbittorrent:
        condition: service_healthy
      sabnzbd:
        condition: service_healthy
      prowlarr:
        condition: service_healthy
    labels:
      - traefik.enable=false
      - homepage.weight=2
      - homepage.group=media
      - homepage.name=radarr
      - homepage.icon=radarr.svg
      - homepage.href=http://192.168.1.102:32783
      - homepage.widget.type=radarr
      - homepage.widget.url=http://192.168.1.102:32783
      - homepage.widget.key=${RADARR_API_KEY}
      - homepage.widget.enableQueue=true


  sabnzbd:
    image: linuxserver/sabnzbd
    container_name: sabnzbd
    <<: *common
    volumes:
      - ./config/sabnzbd:/config
      - /mnt/storage:/data
    ports:
      - 32781:8080
      - 32782:9090
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "curl -f http://localhost:8080 || exit 1"]
    labels:
      - traefik.enable=false
      - homepage.weight=3
      - homepage.group=network
      - homepage.name=sabnzbd
      - homepage.icon=sabnzbd.svg
      - homepage.href=http://192.168.1.102:32781
      - homepage.widget.type=sabnzbd
      - homepage.widget.url=http://192.168.1.102:32781
      - homepage.widget.key=${SABNZBD_API_KEY}
      - homepage.widget.refreshInterval=1000

  qbittorrent:
    image: lscr.io/linuxserver/qbittorrent:4.6.3-r0-ls308
    container_name: qbittorrent
    <<: *common
    environment:
      <<: *env
      WEBUI_PORT: 8080
      DOCKER_MODS: ghcr.io/themepark-dev/theme.park:qbittorrent
    volumes:
      - ./config/qbittorrent:/config
      - /mnt/storage:/data
    ports:
      - 8080:8080
      - 16881:16881
      - 16881:16881/udp
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "curl -f http://localhost:8080 || exit 1"]
    labels:
      - traefik.enable=false
      - homepage.weight=2
      - homepage.group=network
      - homepage.name=qbittorrent
      - homepage.icon=qbittorrent.svg
      - homepage.href=http://192.168.1.102:8080
      - homepage.widget.type=qbittorrent
      - homepage.widget.url=http://192.168.1.102:8080
      - homepage.widget.username=${QBIT_USER}
      - homepage.widget.password=${QBIT_PASS}
      - homepage.widget.refreshInterval=1000

  lidarr:
    image: linuxserver/lidarr
    container_name: lidarr
    <<: *common
    volumes:
      - ./config/lidarr:/config
      - /mnt/storage:/data
    ports:
      - 32786:8686
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "curl -f http://localhost:8686/lidarr/ping || exit 1"]
    depends_on:
      qbittorrent:
        condition: service_healthy
      sabnzbd:
        condition: service_healthy
      prowlarr:
        condition: service_healthy
    labels:
      - traefik.enable=false
      - homepage.weight=4
      - homepage.group=media
      - homepage.name=lidarr
      - homepage.icon=lidarr.svg
      - homepage.href=http://192.168.1.102:32786
      - homepage.widget.type=lidarr
      - homepage.widget.url=http://192.168.1.102:32786
      - homepage.widget.key=${LIDARR_API_KEY}

  readarr:
    image: lscr.io/linuxserver/readarr:develop
    # build:
    #   context: ./dockerfiles
    #   dockerfile: ./readarr
    container_name: readarr
    <<: *common
    volumes:
      - ./config/readarr:/config
      - /mnt/storage:/data
    ports:
      - 8787:8787
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "curl -f http://localhost:8787/readarr/ping || exit 1"]
    depends_on:
      qbittorrent:
        condition: service_healthy
      sabnzbd:
        condition: service_healthy
      prowlarr:
        condition: service_healthy
    labels:
      - traefik.enable=false

  prowlarr:
    image: linuxserver/prowlarr:develop
    container_name: prowlarr
    <<: *common
    volumes:
      - ./config/prowlarr:/config
    ports:
      - 32787:9696
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "curl -f http://localhost:9696/prowlarr/ping || exit 1"]
    depends_on:
      flaresolverr:
        condition: service_healthy
    labels:
      - traefik.enable=false
      - homepage.group=brr
      - homepage.name=prowlarr
      - homepage.icon=prowlarr.svg
      - homepage.href=http://192.168.1.102:32787
      - homepage.widget.type=prowlarr
      - homepage.widget.url=http://192.168.1.102:32787
      - homepage.widget.key=${PROWLARR_API_KEY}

  iperf3:
    image: networkstatic/iperf3
    container_name: iperf3
    ports:
      - 5201:5201
    restart: unless-stopped
    command: -s
    healthcheck:
      <<: *healthcheck
      # not a real healthcheck, only confirms there's a process named iperf3 running
      test: ["CMD-SHELL", "grep iperf3 /proc/*[0-9]*/status || exit 1"]
    labels:
      - traefik.enable=false

  netdata:
    image: netdata/netdata
    container_name: netdata
    environment:
      <<: *env
    restart: unless-stopped
    healthcheck:
      <<: *healthcheck
    network_mode: host
    cap_add:
      - SYS_PTRACE
      - SYS_ADMIN
    security_opt:
      - apparmor:unconfined
    volumes:
      - ./config/netdata/config:/etc/netdata
      - ./config/netdata/data:/var/lib/netdata
      - ./config/netdata/cache:/var/cache/netdata
      - /etc/passwd:/host/etc/passwd:ro
      - /etc/group:/host/etc/group:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /etc/VERSION:/host/etc/os-release:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    labels:
      - traefik.enable=false
      # - homepage.group=network
      # - homepage.name=Netdata
      # - homepage.icon=netdata.svg
      # - homepage.href=http://192.168.1.102:19999
      # - homepage.widget.type=netdata
      # - homepage.widget.url=http://192.168.1.102:19999

  recyclarr:
    image: ghcr.io/recyclarr/recyclarr
    container_name: recyclarr
    <<: *common
    volumes:
      - ./config/recyclarr:/config
    healthcheck:
      <<: *healthcheck
      test: ["CMD-SHELL", "exit 0"] # TODO: figure out what kind of healthcheck we can do
    labels:
      - traefik.enable=false

  autobrr:
    image: ghcr.io/autobrr/autobrr:latest
    container_name: autobrr
    <<: *common
    logging:
      driver: json-file
      options:
        max-file: "1"
        max-size: "10M"
    volumes:
      - ./config/autobrr:/config
    ports:
      - 7474:7474
    healthcheck:
      <<: *healthcheck
      test: ["CMD", "curl", "-f", "http://localhost:7474"]
    depends_on:
      prowlarr:
        condition: service_healthy
      radarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
      lidarr:
        condition: service_healthy
      readarr:
        condition: service_healthy
    labels:
      - traefik.enable=false
      - homepage.group=brr
      - homepage.name=autobrr
      - homepage.icon=autobrr.svg
      - homepage.href=http://192.168.1.102:7474
      - homepage.widget.type=autobrr
      - homepage.widget.url=http://192.168.1.102:7474
      - homepage.widget.key=${AUTOBRR_API_KEY}

  omegabrr:
    container_name: omegabrr
    image: ghcr.io/autobrr/omegabrr:latest
    <<: *common
    healthcheck:
      test: ["CMD", "true"]
    ports:
      - "7441:7441"
    volumes:
      - "./config/omegabrr:/config"
    depends_on:
      autobrr:
        condition: service_healthy
      prowlarr:
        condition: service_healthy
      radarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
      lidarr:
        condition: service_healthy
      readarr:
        condition: service_healthy
    labels:
      - traefik.enable=false

  cross-seed:
    image: crossseed/cross-seed
    container_name: cross-seed
    <<: *common
    healthcheck:
      test: ["CMD", "true"]
    volumes:
      - ./config/cross-seed:/config
      - ./config/qbittorrent/qBittorrent/BT_backup:/torrents:ro # note that this volume can and should be mounted read-only
      - /mnt/storage:/data # this is optional dataDir path (for data-based matching) - will need to mirror your torrent client's path (like Arr's do)
      - /mnt/storage/cross-seeds:/cross-seeds
    command: daemon
    ports:
      - 2468:2468
    depends_on:
      prowlarr:
        condition: service_healthy
      qbittorrent:
        condition: service_healthy
    labels:
      - traefik.enable=false

  qbit_manage:
    container_name: qbit_manage
    image: bobokun/qbit_manage
    <<: *common
    healthcheck:
      test: ["CMD", "true"]
    volumes:
      - ./config/qbit_manage/:/config:rw
      - /mnt/storage:/data
      - ./config/qbittorrent/qBittorrent/BT_backup:/qbittorrent/:ro
    environment:
      <<: *env
      QBT_RUN: "false"
      QBT_SCHEDULE: "5"
      QBT_CONFIG: "config.yml"
      QBT_LOGFILE: "activity.log"
      QBT_CROSS_SEED: "false"
      QBT_RECHECK: "true"
      QBT_CAT_UPDATE: "false"
      QBT_TAG_UPDATE: "true"
      QBT_REM_UNREGISTERED: "false"
      QBT_REM_ORPHANED: "false"
      QBT_TAG_TRACKER_ERROR: "true"
      QBT_TAG_NOHARDLINKS: "true"
      QBT_SHARE_LIMITS: "false"
      QBT_SKIP_CLEANUP: "false"
      QBT_DRY_RUN: "false"
      QBT_LOG_LEVEL: "WARNING"
      QBT_DIVIDER: ""
      QBT_WIDTH: "100"
      QBT_DEBUG: "false"
    depends_on:
      qbittorrent:
        condition: service_healthy
    labels:
      - traefik.enable=false

  flaresolverr:
    # DockerHub mirror flaresolverr/flaresolverr:latest
    image: ghcr.io/flaresolverr/flaresolverr:latest
    container_name: flaresolverr
    <<: *common
    environment:
      <<: *env
      LOG_LEVEL: ${LOG_LEVEL:-info}
      LOG_HTML: ${LOG_HTML:-false}
      CAPTCHA_SOLVER: ${CAPTCHA_SOLVER:-none}
    ports:
      - 8191:8191
    healthcheck:
      <<: *healthcheck
      test: ["CMD", "curl", "-f", "http://localhost:8191"]
    labels:
      - traefik.enable=false


  filebrowser:
    image: filebrowser/filebrowser
    container_name: filebrowser
    <<: *common
    ports:
      - 8888:80
    volumes:
      - /:/srv
      - ./config/filebrowser/filebrowser.db:/database/filebrowser.db
      - ./config/filebrowser/settings.json:/config/settings.json

  wg-easy:
    image: ghcr.io/wg-easy/wg-easy
    container_name: wg-easy
    <<: *common
    environment:
      <<: *env
      # WG_HOST: wg-easy.lab.petalas.dev
      WG_HOST: 140.228.73.215
      PASSWORD: $WG_PASSWORD
      WG_MTU: 1500
    volumes:
      - ./config/wg-easy:/etc/wireguard
    ports:
      - "51820:51820/udp" # WG port
      - "51821:51821/tcp" # web UI port
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    sysctls:
      - net.ipv4.ip_forward=1
      - net.ipv4.conf.all.src_valid_mark=1
    # labels:
    #     - traefik.enable=true
    #     - traefik.http.routers.wg-easy.service=wg-easy
    #     - traefik.http.routers.wg-easy.entrypoints=websecure
    #     - traefik.http.routers.wg-easy.rule=Host(`wg-easy.lab.petalas.dev`)
    #     - traefik.http.routers.wg-easy.tls=true
    #     - traefik.http.services.wg-easy.loadbalancer.server.port=51821
